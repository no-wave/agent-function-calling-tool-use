{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClljNz_me6Uj",
    "tags": []
   },
   "source": [
    "# RAG: Function Calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClljNz_me6Uj",
    "tags": []
   },
   "source": [
    "### 진행 과정\n",
    "\n",
    "이 가이드북은 다음 워크플로우를 안내한다:\n",
    "\n",
    "  - **검색 유틸리티**: 답변을 위해 arXiv에 접근하는 두 함수를 생성한다.\n",
    "  - **에이전트 구성**: 함수 필요성을 평가하고, 필요한 경우 해당 함수를 호출하여 결과를 에이전트에게 다시 제시하는 에이전트 행동을 구축한다.\n",
    "  - **arXiv 대화**: 이 모든 것을 실제 대화에 통합하여 실행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv  \n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ClljNz_me6Uj"
   },
   "source": [
    "### 1\\. 설정 및 라이브러리 설치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q scipy tenacity tiktoken arxiv PyPDF2 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "SJubJL-7e6Ul",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 필요한 라이브러리들을 가져온다.\n",
    "import arxiv\n",
    "import ast \n",
    "import concurrent.futures \n",
    "import json\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import tiktoken \n",
    "from csv import writer\n",
    "\n",
    "from openai import OpenAI\n",
    "from PyPDF2 import PdfReader\n",
    "from scipy import spatial \n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
    "from tqdm import tqdm \n",
    "\n",
    "\n",
    "# 사용할 GPT 모델과 임베딩 모델을 지정한다.\n",
    "\n",
    "EMBEDDING_MODEL= \"text-embedding-3-small\"\n",
    "MODEL = \"gpt-4.1-mini\"\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8hVRPe0e6Ul"
   },
   "source": [
    "### 2\\. 검색 유틸리티 (Search Utilities)\n",
    "\n",
    "먼저 두 함수를 뒷받침할 몇 가지 유틸리티를 설정한다.\n",
    "\n",
    "다운로드된 논문은 특정 디렉토리(여기서는 `./data/papers`)에 저장된다. `arxiv_library.csv` 파일을 생성하여 다운로드된 논문의 임베딩과 세부 정보를 저장하고, 나중에 `summarize_text`를 사용하여 검색할 수 있도록 한다.\n",
    "\n",
    "  * **임베딩(Embedding)이란?** 텍스트(단어, 문장 등)를 컴퓨터가 이해할 수 있는 숫자의 벡터(vector)로 변환하는 기술이다. 이 벡터들은 의미적으로 유사한 텍스트일수록 서로 가까운 위치에 있게 되므로, 텍스트 간의 관련성을 계산하는 데 사용된다.\n",
    "\n",
    "<!-- end list -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qcdra96je6Ul",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "디렉토리 './data/papers'가 이미 존재합니다.\n"
     ]
    }
   ],
   "source": [
    "# 다운로드한 논문을 저장할 디렉토리를 지정한다.\n",
    "directory = '../dataset/papers'\n",
    "\n",
    "# 디렉토리가 이미 존재하는지 확인한다.\n",
    "if not os.path.exists(directory):\n",
    "    # 디렉토리가 없으면, 필요한 모든 중간 디렉토리와 함께 생성한다.\n",
    "    os.makedirs(directory)\n",
    "    print(f\"디렉토리 '{directory}'가 성공적으로 생성되었습니다.\")\n",
    "else:\n",
    "    # 디렉토리가 이미 존재하면, 메시지를 출력한다.\n",
    "    print(f\"디렉토리 '{directory}'가 이미 존재합니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jcUId46oe6Ul",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 다운로드된 논문을 저장할 디렉토리 경로를 설정한다.\n",
    "data_dir = os.path.join(os.curdir, \"data\", \"papers\")\n",
    "# 다운로드된 논문들의 메타데이터(제목, 파일 경로, 임베딩)를 저장할 CSV 파일 경로\n",
    "paper_dir_filepath = \"../dataset/papers/arxiv_library.csv\"\n",
    "\n",
    "# 비어있는 DataFrame을 생성하여 CSV 파일을 초기화한다.\n",
    "df = pd.DataFrame(list())\n",
    "df.to_csv(paper_dir_filepath, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4Vv1LXvAe6Um",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 요청 실패 시 지수적으로 증가하는 대기 시간(최소 1초, 최대 40초)으로 최대 3번 재시도한다.\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def embedding_request(text):\n",
    "    \"\"\"지정된 텍스트에 대한 임베딩을 요청한다.\"\"\"\n",
    "    \n",
    "    response = client.embeddings.create(input=text, model=EMBEDDING_MODEL)\n",
    "    return response\n",
    "\n",
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def get_articles(query, library=paper_dir_filepath, top_k=10):\n",
    "    \"\"\"\n",
    "    이 함수는 사용자의 쿼리를 기반으로 관련성이 높은 상위 k개의 논문을 가져온다.\n",
    "    또한 파일을 다운로드하고, 나중에 read_article_and_summarize 함수가 검색할 수 있도록\n",
    "    arxiv_library.csv에 저장한다.\n",
    "    \"\"\"\n",
    "    client = arxiv.Client()\n",
    "    # 쿼리로 arXiv에서 논문을 검색한다.\n",
    "    search = arxiv.Search(\n",
    "        query = query,\n",
    "        max_results = top_k\n",
    "    )\n",
    "    result_list = []\n",
    "    for result in client.results(search):\n",
    "        result_dict = {}\n",
    "        result_dict.update({\"title\": result.title})\n",
    "        result_dict.update({\"summary\": result.summary})\n",
    "        # 첫 번째 URL을 논문 URL로 사용한다.\n",
    "        result_dict.update({\"article_url\": [x.href for x in result.links][0]})\n",
    "        result_dict.update({\"pdf_url\": [x.href for x in result.links][1]})\n",
    "        result_list.append(result_dict)\n",
    "\n",
    "        # 논문 제목에 대한 임베딩을 생성한다.\n",
    "        response = embedding_request(text=result.title)\n",
    "        # 파일 참조 정보: [논문 제목, 다운로드된 PDF 파일 경로, 제목 임베딩]\n",
    "        file_reference = [\n",
    "            result.title,\n",
    "            result.download_pdf(data_dir),\n",
    "            response.data[0].embedding,\n",
    "        ]\n",
    "\n",
    "        # 파일에 참조 정보를 기록한다.\n",
    "        with open(library, \"a\") as f_object:\n",
    "            writer_object = writer(f_object)\n",
    "            writer_object.writerow(file_reference)\n",
    "            f_object.close()\n",
    "    return result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "2H5dYKWee6Um",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'title': 'Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks',\n",
       " 'summary': 'Retrieval-augmented Generation (RAG) has markedly enhanced the capabilities\\nof Large Language Models (LLMs) in tackling knowledge-intensive tasks. The\\nincreasing demands of application scenarios have driven the evolution of RAG,\\nleading to the integration of advanced retrievers, LLMs and other complementary\\ntechnologies, which in turn has amplified the intricacy of RAG systems.\\nHowever, the rapid advancements are outpacing the foundational RAG paradigm,\\nwith many methods struggling to be unified under the process of\\n\"retrieve-then-generate\". In this context, this paper examines the limitations\\nof the existing RAG paradigm and introduces the modular RAG framework. By\\ndecomposing complex RAG systems into independent modules and specialized\\noperators, it facilitates a highly reconfigurable framework. Modular RAG\\ntranscends the traditional linear architecture, embracing a more advanced\\ndesign that integrates routing, scheduling, and fusion mechanisms. Drawing on\\nextensive research, this paper further identifies prevalent RAG\\npatterns-linear, conditional, branching, and looping-and offers a comprehensive\\nanalysis of their respective implementation nuances. Modular RAG presents\\ninnovative opportunities for the conceptualization and deployment of RAG\\nsystems. Finally, the paper explores the potential emergence of new operators\\nand paradigms, establishing a solid theoretical foundation and a practical\\nroadmap for the continued evolution and practical deployment of RAG\\ntechnologies.',\n",
       " 'article_url': 'http://arxiv.org/abs/2407.21059v1',\n",
       " 'pdf_url': 'http://arxiv.org/pdf/2407.21059v1'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 검색 기능이 잘 작동하는지 테스트한다.\n",
    "result_output = get_articles(\"rag\")\n",
    "result_output[0] # 첫 번째 검색 결과를 확인한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "qNgtLPkxe6Um",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def strings_ranked_by_relatedness(\n",
    "    query: str,\n",
    "    df: pd.DataFrame,\n",
    "    relatedness_fn=lambda x, y: 1 - spatial.distance.cosine(x, y), # 코사인 유사도 계산 함수\n",
    "    top_n: int = 100,\n",
    ") -> list[str]:\n",
    "    \n",
    "    \"\"\"가장 관련성이 높은 순으로 정렬된 문자열과 관련도 목록을 반환한다.\"\"\"\n",
    "    \n",
    "    # 쿼리에 대한 임베딩을 생성한다.\n",
    "    query_embedding_response = embedding_request(query)\n",
    "    query_embedding = query_embedding_response.data[0].embedding\n",
    "\n",
    "    # 데이터프레임의 각 항목과 쿼리 간의 관련도를 계산한다.\n",
    "    strings_and_relatednesses = [\n",
    "        (row[\"filepath\"], relatedness_fn(query_embedding, row[\"embedding\"]))\n",
    "        for i, row in df.iterrows()\n",
    "    ]\n",
    "    # 관련도(유사도)가 높은 순으로 정렬한다.\n",
    "    strings_and_relatednesses.sort(key=lambda x: x[1], reverse=True)\n",
    "    strings, relatednesses = zip(*strings_and_relatednesses)\n",
    "    return strings[:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Qq1reJd1e6Um",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def read_pdf(filepath):\n",
    "    \"\"\"PDF 파일 경로를 받아 PDF의 내용을 문자열로 반환한다.\"\"\"\n",
    "    # pdf reader 객체 생성\n",
    "    reader = PdfReader(filepath)\n",
    "    pdf_text = \"\"\n",
    "    page_number = 0\n",
    "    for page in reader.pages:\n",
    "        page_number += 1\n",
    "        pdf_text += page.extract_text() + f\"\\nPage Number: {page_number}\"\n",
    "    return pdf_text\n",
    "\n",
    "\n",
    "def create_chunks(text, n, tokenizer):\n",
    "    \"\"\"제공된 텍스트에서 n-크기의 청크(조각)를 연속적으로 반환한다.\"\"\"\n",
    "    tokens = tokenizer.encode(text)\n",
    "    i = 0\n",
    "    while i < len(tokens):\n",
    "        # 0.5*n에서 1.5*n 토큰 범위 내에서 가장 가까운 문장 끝을 찾는다.\n",
    "        j = min(i + int(1.5 * n), len(tokens))\n",
    "        while j > i + int(0.5 * n):\n",
    "            # 토큰을 디코딩하여 마침표나 줄바꿈 문자가 있는지 확인한다.\n",
    "            chunk = tokenizer.decode(tokens[i:j])\n",
    "            if chunk.endswith(\".\") or chunk.endswith(\"\\n\"):\n",
    "                break\n",
    "            j -= 1\n",
    "        # 문장 끝을 찾지 못하면 n 토큰을 청크 크기로 사용한다.\n",
    "        if j == i + int(0.5 * n):\n",
    "            j = min(i + n, len(tokens))\n",
    "        yield tokens[i:j]\n",
    "        i = j\n",
    "\n",
    "\n",
    "def extract_chunk(content, template_prompt):\n",
    "    \"\"\"이 함수는 입력 콘텐츠에 프롬프트를 적용한다. 이 경우 요약된 텍스트 청크를 반환한다.\"\"\"\n",
    "    prompt = template_prompt + content\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL, messages=[{\"role\": \"user\", \"content\": prompt}], temperature=0\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def summarize_text(query):\n",
    "    \"\"\"\n",
    "    이 함수는 다음을 수행한다:\n",
    "    - 임베딩을 포함한 arxiv_library.csv 파일을 읽는다.\n",
    "    - 사용자 쿼리와 가장 유사한 파일을 찾는다.\n",
    "    - 파일에서 텍스트를 스크랩하여 청크로 나눈다.\n",
    "    - 각 청크를 병렬로 요약한다.\n",
    "    - 최종 요약을 수행하고 이를 사용자에게 반환한다.\n",
    "    \"\"\"\n",
    "    # 재귀적 요약이 입력 논문에 어떻게 접근해야 하는지 지시하는 프롬프트\n",
    "    summary_prompt = \"\"\"학술 논문에서 발췌한 이 텍스트를 요약하고, 핵심 포인트와 근거를 추출해주세요.\\n\\n내용:\"\"\"\n",
    "\n",
    "    # 라이브러리가 비어 있으면(아직 검색이 수행되지 않음), 먼저 검색을 수행하고 결과를 다운로드\n",
    "    library_df = pd.read_csv(paper_dir_filepath, header=None).reset_index(drop=True)\n",
    "    if len(library_df) == 0:\n",
    "        print(\"아직 검색된 논문이 없습니다. 먼저 다운로드합니다.\")\n",
    "        get_articles(query)\n",
    "        print(\"논문 다운로드 완료, 계속 진행합니다.\")\n",
    "        library_df = pd.read_csv(paper_dir_filepath, header=None).reset_index(drop=True)\n",
    "    else:\n",
    "        print(f\"기존 논문 발견... 논문 수: {len(library_df)}\")\n",
    "\n",
    "    library_df.columns = [\"title\", \"filepath\", \"embedding\"]\n",
    "    # 문자열 형태의 임베딩을 실제 리스트 객체로 변환한다.\n",
    "    library_df[\"embedding\"] = library_df[\"embedding\"].apply(ast.literal_eval)\n",
    "    # 쿼리와 가장 관련성이 높은 논문 파일 경로를 찾는다.\n",
    "    strings = strings_ranked_by_relatedness(query, library_df, top_n=1)\n",
    "\n",
    "    print(\"논문에서 텍스트를 청크로 나누는 중\")\n",
    "    pdf_text = read_pdf(strings[0])\n",
    "\n",
    "    # 토크나이저 초기화\n",
    "    tokenizer = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    results = \"\"\n",
    "\n",
    "    # 문서를 1500 토큰 청크로 나눈다.\n",
    "    chunks = create_chunks(pdf_text, 1500, tokenizer)\n",
    "    text_chunks = [tokenizer.decode(chunk) for chunk in chunks]\n",
    "    print(\"각 텍스트 청크 요약 중\")\n",
    "\n",
    "    # 요약을 병렬로 처리한다.\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=len(text_chunks)) as executor:\n",
    "        futures = [executor.submit(extract_chunk, chunk, summary_prompt) for chunk in text_chunks]\n",
    "        with tqdm(total=len(text_chunks)) as pbar:\n",
    "            for _ in concurrent.futures.as_completed(futures):\n",
    "                pbar.update(1)\n",
    "        for future in futures:\n",
    "            data = future.result()\n",
    "            results += data\n",
    "\n",
    "    # 최종 요약\n",
    "    print(\"전체 요약으로 종합하는 중\")\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"학술 논문에서 추출한 이 핵심 포인트 모음에서 종합된 요약을 작성해주세요.\n",
    "                        요약은 핵심 주장, 결론 및 증거를 강조하고 사용자의 질문에 답변해야 합니다.\n",
    "                        사용자 질문: {query}\n",
    "                        요약은 '핵심 주장', '증거', '결론'이라는 제목 아래 글머리 기호 목록으로 구조화되어야 합니다.\n",
    "                        핵심 포인트:\\n{results}\\n요약:\\n\"\"\",\n",
    "            }\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "zFMu2-3ye6Um",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기존 논문 발견... 논문 수: 20\n",
      "논문에서 텍스트를 청크로 나누는 중\n",
      "각 텍스트 청크 요약 중\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:17<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 요약으로 종합하는 중\n"
     ]
    }
   ],
   "source": [
    "# summarize_text 함수가 작동하는지 테스트한다.\n",
    "chat_test_response = summarize_text(\"modular rag\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "fHbhUouLe6Um",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## 핵심 주장\n",
       "\n",
       "- **Modular RAG(모듈화된 Retrieval-Augmented Generation) 프레임워크는 기존 RAG 시스템의 한계를 극복하고, 복잡한 쿼리와 다양한 데이터 소스에 유연하고 확장 가능하게 대응할 수 있도록 설계된 혁신적 패러다임이다.**\n",
       "- Modular RAG는 RAG 시스템을 독립적 모듈과 연산자(operator)로 분해하여, 라우팅, 스케줄링, 융합 등 다양한 워크플로우를 구성할 수 있게 하며, 기존 Naive RAG와 Advanced RAG를 포괄하는 상위 개념으로 발전시킨다.\n",
       "- 이 프레임워크는 RAG 시스템의 설계, 관리, 유지보수를 용이하게 하고, 새로운 모듈과 연산자 개발을 촉진하여 RAG 기술의 이론적·실용적 발전을 견인한다.\n",
       "\n",
       "## 증거\n",
       "\n",
       "- **기존 RAG 한계**: Naive RAG는 단순 쿼리-문서 유사도 기반 검색에 의존해 복잡한 쿼리 처리에 취약하며, Advanced RAG도 검색 정확도 향상에 집중했으나 데이터 통합, 시스템 해석성, 유지보수 문제는 여전함. (예: “Shallow Understanding of Queries”, “Retrieval Redundancy and Noise” 문제)\n",
       "- **Modular RAG 구조**: 3계층(모듈, 서브모듈, 연산자)로 RAG 시스템을 분해하고, 라우팅·스케줄링·융합 메커니즘을 포함해 비선형적이고 유연한 워크플로우를 구성함. 기존 RAG는 Modular RAG의 특수 사례임. (예: “Modular RAG system consists of multiple independent yet tightly coordinated modules”)\n",
       "- **유연성 및 확장성**: 다양한 데이터 유형(비정형 텍스트, 테이블, 지식 그래프 등)과 복잡한 작업 시나리오에 맞게 모듈과 연산자를 조합 가능하며, 유지보수와 이해도가 향상됨. (예: “enhances the system’s flexibility and scalability”, “strengthens the system’s maintainability and comprehensibility”)\n",
       "- **실제 적용 사례**: 인덱싱 모듈에서 문서 분할, 청크 최적화, 계층적 구조화, 메타데이터 부착 등을 통해 검색 정확도와 효율성을 높임. (예: “Chunk Optimization”, “Hierarchical Index”, “Metadata Attachment”)\n",
       "- **다양한 RAG 흐름 패턴 지원**: 선형, 조건부, 분기, 루프 패턴 등 다양한 워크플로우를 지원하며, LLM 판단 및 제어, 지식 기반 스케줄링, 다중 분기 결과 융합 기법을 포함함.\n",
       "- **미세조정과 검증 기법**: 검색기와 생성기의 미세조정, LLM-Critique, 외부 지식 기반 검증, 강화학습 등을 통해 성능과 신뢰성을 높임.\n",
       "- **문헌 및 최신 연구 동향**: RAG와 LLM 결합, 계획 기반 검색, 자기 비판적 학습, 다중 에이전트 협업, 환각 문제 완화 등 다양한 최신 연구가 Modular RAG의 이론적·실용적 기반을 뒷받침함.\n",
       "\n",
       "## 결론\n",
       "\n",
       "- Modular RAG는 기존 RAG 시스템의 단점을 극복하고, 복잡하고 다양한 실제 응용 요구에 맞춰 RAG 시스템을 유연하고 확장 가능하게 설계할 수 있는 혁신적 프레임워크이다.\n",
       "- 모듈화된 설계는 시스템의 유지보수성과 해석성을 크게 향상시키며, 새로운 모듈과 연산자 개발을 촉진하여 RAG 기술의 지속적 발전을 가능하게 한다.\n",
       "- 특히, 인덱싱 단계의 최적화, 다양한 흐름 패턴 지원, LLM 기반 판단 및 제어, 미세조정과 검증 기법의 통합적 활용이 RAG 성능 향상의 핵심 요소임을 입증한다.\n",
       "- Modular RAG는 RAG 기술의 이론적 발전과 실용적 적용을 위한 강력한 기반을 제공하며, 향후 복잡한 지식 집약적 NLP 작업과 대규모 시스템 구축에 필수적인 설계 패러다임으로 자리매김할 전망이다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 테스트 응답의 내용을 마크다운 형식으로 표시한다.\n",
    "display(Markdown(chat_test_response.choices[0].message.content))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWRASVnCe6Um"
   },
   "source": [
    "### 3\\. 에이전트 구성 (Configure Agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_vp9UIp_e6Um",
    "tags": []
   },
   "outputs": [],
   "source": [
    "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
    "def chat_completion_request(messages, functions=None, model=MODEL):\n",
    "    \"\"\"ChatCompletion API에 요청을 보내는 헬퍼 함수\"\"\"\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            # tools 매개변수가 최신 API에서는 권장되지만, 여기서는 functions를 사용한다.\n",
    "            functions=functions,\n",
    "        )\n",
    "        return response\n",
    "    except Exception as e:\n",
    "        print(\"ChatCompletion 응답을 생성할 수 없습니다.\")\n",
    "        print(f\"예외: {e}\")\n",
    "        return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "o_ke60Bge6Um",
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Conversation:\n",
    "    \"\"\"대화 기록을 관리하는 클래스\"\"\"\n",
    "    def __init__(self):\n",
    "        self.conversation_history = []\n",
    "\n",
    "    def add_message(self, role, content):\n",
    "        message = {\"role\": role, \"content\": content}\n",
    "        self.conversation_history.append(message)\n",
    "\n",
    "    def display_conversation(self, detailed=False):\n",
    "        role_to_color = {\n",
    "            \"system\": \"red\",\n",
    "            \"user\": \"green\",\n",
    "            \"assistant\": \"blue\",\n",
    "            \"function\": \"magenta\",\n",
    "        }\n",
    "        for message in self.conversation_history:\n",
    "            print(\n",
    "                colored(\n",
    "                    f\"{message['role']}: {message['content']}\\n\\n\",\n",
    "                    role_to_color[message[\"role\"]],\n",
    "                )\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jRG1WZXAe6Um",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get_articles와 read_article_and_summarize 함수를 모델에게 알려주기 위한 명세(schema)\n",
    "arxiv_functions = [\n",
    "    {\n",
    "        \"name\": \"get_articles\",\n",
    "        \"description\": \"사용자 질문에 답하기 위해 arXiv에서 학술 논문을 가져오려면 이 함수를 사용하세요.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"JSON 형식의 사용자 쿼리입니다. 응답은 요약되어야 하며 논문 URL 참조를 포함해야 합니다.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"read_article_and_summarize\",\n",
    "        \"description\": \"사용자를 위해 전체 논문을 읽고 요약을 제공하려면 이 함수를 사용하세요. 대화에서 get_articles가 호출되기 전에는 절대로 이 함수를 호출해서는 안 됩니다.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"사용자 쿼리를 기반으로 한 논문의 일반 텍스트 설명입니다.\",\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "        },\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "mKTDknzke6Um",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chat_completion_with_function_execution(messages, functions=[None]):\n",
    "    \"\"\"이 함수는 함수를 추가하는 옵션으로 ChatCompletion API를 호출한다.\"\"\"\n",
    "    response = chat_completion_request(messages, functions)\n",
    "    full_message = response.choices[0]\n",
    "    # 모델이 함수 호출을 결정했는지 확인한다.\n",
    "    if full_message.finish_reason == \"function_call\":\n",
    "        print(\"함수 생성이 요청되어 함수를 호출합니다.\")\n",
    "        return call_arxiv_function(messages, full_message)\n",
    "    else:\n",
    "        print(\"함수가 필요하지 않아 사용자에게 응답합니다.\")\n",
    "        return response\n",
    "\n",
    "\n",
    "def call_arxiv_function(messages, full_message):\n",
    "    \"\"\"\n",
    "    모델이 필요하다고 판단할 때 함수 호출을 실행하는 함수.\n",
    "    현재는 이 if문에 절을 추가하여 확장된다.\n",
    "    \"\"\"\n",
    "    function_name = full_message.message.function_call.name\n",
    "\n",
    "    if function_name == \"get_articles\":\n",
    "        try:\n",
    "            # 모델이 생성한 인자(arguments)를 파싱한다.\n",
    "            parsed_output = json.loads(full_message.message.function_call.arguments)\n",
    "            print(\"검색 결과를 가져오는 중\")\n",
    "            # 실제 함수를 실행한다.\n",
    "            results = get_articles(parsed_output[\"query\"])\n",
    "        except Exception as e:\n",
    "            print(parsed_output)\n",
    "            print(\"함수 실행 실패\")\n",
    "            print(f\"오류 메시지: {e}\")\n",
    "        # 함수 실행 결과를 대화 기록에 추가한다.\n",
    "        messages.append({\n",
    "            \"role\": \"function\",\n",
    "            \"name\": function_name,\n",
    "            \"content\": str(results),\n",
    "        })\n",
    "        try:\n",
    "            print(\"검색 결과를 요약하는 중\")\n",
    "            # 함수 실행 결과를 바탕으로 모델에게 다시 자연어 응답을 생성하도록 요청한다.\n",
    "            response = chat_completion_request(messages)\n",
    "            return response\n",
    "        except Exception as e:\n",
    "            print(type(e))\n",
    "            raise Exception(\"함수 채팅 요청 실패\")\n",
    "\n",
    "    elif function_name == \"read_article_and_summarize\":\n",
    "        parsed_output = json.loads(full_message.message.function_call.arguments)\n",
    "        print(\"논문을 찾고 읽는 중\")\n",
    "        summary = summarize_text(parsed_output[\"query\"])\n",
    "        return summary\n",
    "    else:\n",
    "        raise Exception(\"함수가 존재하지 않으므로 호출할 수 없습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Se3qkeje6Um"
   },
   "source": [
    "### 4\\. arXiv 대화 (arXiv Conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "dn54A4Hie6Um",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 시스템 메시지로 시작한다.\n",
    "paper_system_message = \"\"\"당신은 arXivGPT이며, 사용자 질문에 답변하기 위해 학술 논문을 가져오는 유용한 조수입니다.\n",
    "당신은 고객이 질문에 대한 답을 찾기 위해 어떤 논문을 읽을지 결정할 수 있도록 논문을 명확하게 요약합니다.\n",
    "사용자가 논문의 이름을 이해하고 클릭하여 접근할 수 있도록 항상 article_url과 제목을 제공합니다.\n",
    "시작하세요!\"\"\"\n",
    "\n",
    "# 대화 객체를 생성하고 시스템 메시지를 추가한다.\n",
    "paper_conversation = Conversation()\n",
    "paper_conversation.add_message(\"system\", paper_system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "XF0UW2yLe6Un",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "함수 생성이 요청되어 함수를 호출합니다.\n",
      "검색 결과를 가져오는 중\n",
      "검색 결과를 요약하는 중\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Modular RAG에 대한 가장 관련성 높은 논문은 다음과 같습니다:\n",
       "\n",
       "논문 제목: Modular RAG: Transforming RAG Systems into LEGO-like Reconfigurable Frameworks  \n",
       "URL: http://arxiv.org/abs/2407.21059v1  \n",
       "요약:  \n",
       "Retrieval-augmented Generation (RAG)은 대규모 언어 모델(LLM)의 지식집약적 과제 처리 능력을 크게 향상시켰습니다. 그러나 RAG 시스템이 점차 복잡해지고 빠르게 발전함에 따라 기존의 \"검색 후 생성(retrieve-then-generate)\"이라는 기본 RAG 패러다임으로 모든 방법을 통합하기 어려워졌습니다.  \n",
       "이 논문에서는 기존 RAG의 한계를 지적하고, RAG 시스템을 독립 모듈과 특수화된 연산자로 분해하는 '모듈러 RAG(Modular RAG)' 프레임워크를 제안합니다. Modular RAG는 전통적인 선형 아키텍처를 넘어 라우팅, 스케줄링, 융합 메커니즘을 통합하는 재구성 가능하고 유연한 디자인을 특징으로 합니다.  \n",
       "특히 Modular RAG는 다양한 RAG 패턴(선형, 조건부, 분기, 루프)을 분석하며, RAG 시스템 설계 및 배포에 혁신적인 기회를 제공합니다. 이 접근법은 RAG의 지속적인 발전과 실제 적용을 위한 이론적 토대와 실용적 로드맵을 제시합니다.\n",
       "\n",
       "요약하자면, Modular RAG는 기존의 통합된 RAG 프로세스를 LEGO 블록처럼 분할 가능한 독립 모듈로 바꾸어, 각 모듈이 독립적으로 구성되고, 라우팅과 스케줄링, 데이터 융합 등의 복잡한 프로세스를 지원함으로써 RAG 시스템을 더 유연하고 재사용 가능하게 만드는 혁신적인 프레임워크입니다.\n",
       "\n",
       "더 자세한 내용을 원하시면 위 URL에서 논문 전문을 참고해 보세요."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 사용자 메시지를 추가한다.\n",
    "paper_conversation.add_message(\"user\", \"안녕하세요, Modular RAG는 어떻게 작동하나요?\")\n",
    "chat_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, \n",
    "    functions=arxiv_functions\n",
    ")\n",
    "assistant_message = chat_response.choices[0].message.content\n",
    "paper_conversation.add_message(\"assistant\", assistant_message)\n",
    "display(Markdown(assistant_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3Ug6Pw_fe6Un"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "함수 생성이 요청되어 함수를 호출합니다.\n",
      "논문을 찾고 읽는 중\n",
      "기존 논문 발견... 논문 수: 30\n",
      "논문에서 텍스트를 청크로 나누는 중\n",
      "각 텍스트 청크 요약 중\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:18<00:00,  1.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 요약으로 종합하는 중\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## 핵심 주장\n",
       "\n",
       "- **Modular RAG(모듈러 RAG)는 기존 RAG 시스템의 한계를 극복하고, LEGO 블록처럼 조립 가능한 독립적 모듈과 연산자(operator)로 RAG 시스템을 재구성하는 혁신적 프레임워크이다.**  \n",
       "- 기존 Naive RAG는 단순 선형 구조로 복잡한 쿼리와 다양한 데이터 유형 처리에 취약하며, Advanced RAG는 검색 정확도는 개선했으나 시스템 복잡성과 유지보수 문제를 내포한다.  \n",
       "- Modular RAG는 라우팅, 스케줄링, 융합 등 다양한 기능을 모듈화하여 유연하고 확장 가능한 RAG 아키텍처를 제공하며, 선형, 조건부, 분기, 반복 등 다양한 RAG 흐름 패턴을 포괄한다.  \n",
       "- 모듈화된 설계는 시스템의 유지보수성, 해석 가능성, 제어성을 크게 향상시키고, 새로운 기술과 방법론과의 호환성 및 확장성을 보장한다.  \n",
       "- Modular RAG는 RAG 기술의 이론적 토대와 실용적 발전을 위한 로드맵을 제시하며, 복잡한 실제 응용 시나리오에 적합한 프레임워크임을 입증한다.\n",
       "\n",
       "## 증거\n",
       "\n",
       "- **기존 RAG 한계**: Naive RAG는 쿼리-문서 유사도 기반 단순 검색에 의존해 복잡한 쿼리 처리에 취약하고, 중복 및 노이즈가 많아 LLM의 정확한 정보 추출을 방해함.  \n",
       "- **Advanced RAG 한계**: 쿼리 재작성, 결과 재순위화 등으로 검색 정확도는 향상되었으나, 복잡한 워크플로우 관리와 이질적 데이터 통합에 어려움 존재.  \n",
       "- **Modular RAG 구조**: RAG 시스템을 모듈, 서브모듈, 연산자 3단계로 분해하고, 이를 계산 그래프 형태로 표현하여 다양한 RAG 패턴(선형, 조건부, 분기, 반복)을 지원함.  \n",
       "- **유연성 및 확장성**: 모듈과 연산자의 조합으로 다양한 데이터 소스와 작업 시나리오에 맞게 시스템을 맞춤화 가능하며, 독립적 모듈 설계로 유지보수 및 디버깅이 용이함.  \n",
       "- **실제 적용 사례**: DR-RAG(2단계 검색 및 분류기 조합), PlanRAG(사전 계획 및 재계획 단계 도입), Multi-Head RAG(다중 주의층 활용) 등 다양한 사례에서 모듈러 RAG의 유연성과 강력함 입증.  \n",
       "- **RAG 흐름 패턴**: 선형, 조건부, 분기, 루프(반복/재귀) 패턴을 체계적으로 분석하고, 각 패턴에 맞는 모듈 조합과 제어 메커니즘을 제시함.  \n",
       "- **미세조정 및 검증 기법**: 검색기 및 생성기 미세조정, LLM 자체 평가, 외부 지식 기반 검증 등 다양한 성능 향상 및 신뢰성 확보 기법과의 통합 가능성.\n",
       "\n",
       "## 결론\n",
       "\n",
       "- Modular RAG는 RAG 시스템을 LEGO 블록처럼 독립적이고 재조합 가능한 모듈과 연산자로 분해하여, 복잡한 쿼리와 다양한 데이터 유형에 유연하게 대응할 수 있는 혁신적 프레임워크이다.  \n",
       "- 이를 통해 기존 RAG의 한계를 극복하고, 유지보수성, 해석 가능성, 제어성, 확장성을 대폭 향상시킨다.  \n",
       "- Modular RAG는 다양한 RAG 흐름 패턴(선형, 조건부, 분기, 반복)을 포괄하며, 실제 응용에서의 복잡한 워크플로우 관리와 최적화 문제를 해결한다.  \n",
       "- 또한, 미세조정, 검증, 동적 라우팅 및 스케줄링 등 최신 기법과의 높은 호환성을 갖추어 RAG 기술의 이론적·실용적 발전을 위한 견고한 기반을 제공한다.  \n",
       "- 따라서 Modular RAG는 RAG 시스템 설계 및 구현의 새로운 표준으로 자리매김할 가능성이 크며, 향후 연구 및 산업적 응용에서 핵심적인 역할을 할 것으로 기대된다."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 두 번째 도구를 사용하도록 유도하는 다른 사용자 메시지를 추가한다.\n",
    "paper_conversation.add_message(\n",
    "    \"user\",\n",
    "    \"RAG 파이프라인 논문을 읽고 요약해 주시겠어요?\",\n",
    ")\n",
    "updated_response = chat_completion_with_function_execution(\n",
    "    paper_conversation.conversation_history, \n",
    "    functions=arxiv_functions\n",
    ")\n",
    "\n",
    "display(Markdown(updated_response.choices[0].message.content))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "lecture",
   "language": "python",
   "name": "lecture"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
